{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5z1baDb31A_w"
      },
      "outputs": [],
      "source": [
        "%%capture --no-stderr\n",
        "%pip install langchain langchain-openai langchain-openai langchain_chroma langchain-text-splitters langchain_community\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BGw0fRfd1fIv",
        "outputId": "9f6cd355-b1d5-4658-c3df-ac5357b7a70f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openai in /usr/local/lib/python3.12/dist-packages (1.100.0)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from openai) (4.10.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai) (1.9.0)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from openai) (0.28.1)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from openai) (0.10.0)\n",
            "Requirement already satisfied: pydantic<3,>=1.9.0 in /usr/local/lib/python3.12/dist-packages (from openai) (2.11.7)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.12/dist-packages (from openai) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.12/dist-packages (from openai) (4.14.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->openai) (3.10)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai) (2025.8.3)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->openai) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3,>=1.9.0->openai) (0.4.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import getpass\n",
        "import os\n",
        "\n",
        "os.environ['OPENAI_API_KEY'] = getpass.getpass(prompt=\"OpenAI API 키를 입력해주세요 (입력 시 문자 숨김): \")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UyVLo4Kp1lR1",
        "outputId": "7d86699f-d9fb-414b-8536-68aa4713a38c"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "OpenAI API 키를 입력해주세요 (입력 시 문자 숨김): ··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import getpass\n",
        "import os\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = getpass.getpass()\n",
        "\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "llm = ChatOpenAI(model=\"gpt-4o\")\n",
        "resp = llm.invoke(\"간단히 자기소개 해줘\")\n",
        "print(resp.content)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7cjMUSbi2fkb",
        "outputId": "f24d7283-187d-4365-965b-bbef391dae4e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "··········\n",
            "안녕하세요! 저는 OpenAI에서 개발한 AI 언어 모델입니다. 여러분의 질문에 답변하고 정보를 제공하기 위해 만들어졌습니다. 다양한 주제에 대해 이야기 할 수 있으며, 필요에 따라 도움을 드릴 수 있도록 최선을 다하겠습니다. 어떤 도움이 필요하신가요?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import getpass\n",
        "import os\n",
        "\n",
        "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
        "os.environ[\"LANGCHAIN_API_KEY\"] = getpass.getpass()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BZtjS7rA7DcG",
        "outputId": "7bbaa225-8001-4987-c237-95127bffb575"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import hub\n",
        "\n",
        "prompt = hub.pull(\"rlm/rag-prompt\")\n",
        "\n",
        "example_messages = prompt.invoke(\n",
        "    {\"context\": \"filler context\", \"question\": \"filler question\"}\n",
        ").to_messages()\n",
        "\n",
        "example_messages"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HFMxQnoy7aFC",
        "outputId": "ac8a9ade-84fb-4f5a-9bcc-9416698c6808"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[HumanMessage(content=\"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\\nQuestion: filler question \\nContext: filler context \\nAnswer:\", additional_kwargs={}, response_metadata={})]"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "resp = llm.invoke(\"간단히 자기소개 해줘\")\n",
        "print(resp.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aflWw7oa-eWX",
        "outputId": "4fbb3f7f-41d3-4059-dc72-b3a5a592b183"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "안녕하세요! 저는 여러분에게 정보를 제공하고 질문에 답변하기 위해 생성된 AI 어시스턴트입니다. 다양한 주제에 대한 정보를 제공하고 여러분의 질문에 최대한 도움을 드리기 위해 노력하고 있습니다. 무엇을 도와드릴까요?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List  # ← List 타입 추가\n",
        "from langchain_core.documents import Document\n",
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "\n",
        "def task1_load() -> List[Document]:\n",
        "    urls = [\n",
        "        \"https://lilianweng.github.io/posts/2023-06-23-agent/\",\n",
        "        \"https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/\",\n",
        "        \"https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/\",\n",
        "    ]\n",
        "    loader = WebBaseLoader(urls)\n",
        "    docs = loader.load()\n",
        "    return docs\n",
        "\n",
        "docs = task1_load()\n"
      ],
      "metadata": {
        "id": "BE5btkPO-_vM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "def task2_split(docs: List[Document],\n",
        "                chunk_size: int = 1200,\n",
        "                chunk_overlap: int = 200) -> List[Document]:\n",
        "    splitter = RecursiveCharacterTextSplitter(\n",
        "        chunk_size=chunk_size,\n",
        "        chunk_overlap=chunk_overlap\n",
        "    )\n",
        "    return splitter.split_documents(docs)\n"
      ],
      "metadata": {
        "id": "Ja8QHQcr_QIJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_chroma import Chroma\n",
        "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
        "\n",
        "def task3_build_vectorstore(chunks: List[Document]) -> Chroma:\n",
        "    embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
        "    vs = Chroma.from_documents(\n",
        "        documents=chunks,\n",
        "        embedding=embeddings,\n",
        "        persist_directory=\"./chroma_rag_demo\",\n",
        "    )\n",
        "\n",
        "    return vs\n"
      ],
      "metadata": {
        "id": "zHzjRMXd_SiV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def task4_retrieve(vs: Chroma, query: str = \"agent memory\") -> List[Document]:\n",
        "    retriever = vs.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 6})\n",
        "    return retriever.invoke(query)"
      ],
      "metadata": {
        "id": "Kjr_eSaW_f7W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.documents import Document\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_core.output_parsers import JsonOutputParser, StrOutputParser\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_core.output_parsers import JsonOutputParser\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "def build_relevance_chain(llm: ChatOpenAI):\n",
        "    \"\"\"retrieved chunk가 query와 관련 있는지 yes/no(JSON)로만 판단\"\"\"\n",
        "    parser = JsonOutputParser()\n",
        "    prompt = PromptTemplate(\n",
        "        template=(\n",
        "            \"You are a strict relevance judge for a RAG system.\\n\"\n",
        "            \"Decide if the given context is relevant to the question.\\n\"\n",
        "            \"Respond ONLY with EXACTLY ONE of the following two JSON lines:\\n\"\n",
        "            \"{{\\\"relevance\\\": \\\"yes\\\"}}\\n\"\n",
        "            \"{{\\\"relevance\\\": \\\"no\\\"}}\\n\"\n",
        "            \"# Question:\\n{question}\\n\\n\"\n",
        "            \"# Context:\\n{context}\\n\"\n",
        "        ),\n",
        "        input_variables=[\"question\", \"context\"],\n",
        "    )\n",
        "    return prompt | llm | parser\n",
        "\n",
        "\n",
        "def build_hallucination_chain(llm: ChatOpenAI):\n",
        "    \"\"\"답변이 context로 뒷받침되는지 yes/no(JSON)로만 판단\"\"\"\n",
        "    parser = JsonOutputParser()\n",
        "    prompt = PromptTemplate(\n",
        "        template=(\n",
        "            \"You are a strict hallucination detector.\\n\"\n",
        "            \"Judge whether the Answer contains claims NOT supported by the Context.\\n\"\n",
        "            \"Respond ONLY with EXACTLY ONE of the following two JSON lines:\\n\"\n",
        "            \"{{\\\"hallucination\\\": \\\"yes\\\"}}\\n\"\n",
        "            \"{{\\\"hallucination\\\": \\\"no\\\"}}\\n\"\n",
        "            \"# Answer:\\n{answer}\\n\\n\"\n",
        "            \"# Context:\\n{context}\\n\"\n",
        "        ),\n",
        "        input_variables=[\"answer\", \"context\"],\n",
        "    )\n",
        "    return prompt | llm | parser\n",
        "\n"
      ],
      "metadata": {
        "id": "X1-uZGHK_imL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def task6_cases():\n",
        "    yes_docs = [Document(page_content=\"Agent memory stores user preferences...\")]\n",
        "    no_docs = [Document(page_content=\"Paris is capital of France.\")]\n",
        "    return {\"YES_ALL\": yes_docs, \"NO_ALL\": no_docs}\n"
      ],
      "metadata": {
        "id": "Q8kQ3zrw_k0v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from dataclasses import dataclass\n",
        "from typing import List, Dict, Any\n",
        "\n",
        "@dataclass\n",
        "class EvalResult:\n",
        "    relevance_yes_for_all: bool\n",
        "    relevance_no_for_all: bool\n",
        "    details: List[Dict[str, Any]]\n",
        "\n",
        "def task7_verify_relevance(llm: ChatOpenAI) -> EvalResult:\n",
        "    chain = build_relevance_chain(llm)\n",
        "    cases = task6_cases()\n",
        "    q = \"agent memory\"\n",
        "\n",
        "    details = []\n",
        "    yes_flags, no_flags = [], []\n",
        "\n",
        "    # YES 케이스 검증\n",
        "    for d in cases[\"YES_ALL\"]:\n",
        "        res = chain.invoke({\"question\": q, \"context\": d.page_content})\n",
        "        details.append({\"case\": \"YES_ALL\", \"doc\": d.page_content[:80], \"res\": res})\n",
        "        yes_flags.append(res.get(\"relevance\") == \"yes\")\n",
        "\n",
        "    # NO 케이스 검증\n",
        "    for d in cases[\"NO_ALL\"]:\n",
        "        res = chain.invoke({\"question\": q, \"context\": d.page_content})\n",
        "        details.append({\"case\": \"NO_ALL\", \"doc\": d.page_content[:80], \"res\": res})\n",
        "        no_flags.append(res.get(\"relevance\") == \"no\")\n",
        "\n",
        "    return EvalResult(\n",
        "        relevance_yes_for_all=all(yes_flags),\n",
        "        relevance_no_for_all=all(no_flags),\n",
        "        details=details\n",
        "    )\n",
        "\n"
      ],
      "metadata": {
        "id": "49sfhKKn_k2W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_rag_answer_chain(llm: ChatOpenAI):\n",
        "    answer_prompt = PromptTemplate(\n",
        "        template=\"Context:\\n{context}\\n\\nQuestion: {question}\\n\"\n",
        "                 \"→ Context를 기반으로 답변하라.\",\n",
        "        input_variables=[\"context\", \"question\"]\n",
        "    )\n",
        "    chain = (\n",
        "        {\"context\": RunnablePassthrough(), \"question\": RunnablePassthrough()}\n",
        "        | answer_prompt\n",
        "        | llm\n",
        "        | StrOutputParser()\n",
        "    )\n",
        "    return chain\n"
      ],
      "metadata": {
        "id": "mAlUmGQ1_onn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List\n",
        "from langchain_core.documents import Document\n",
        "\n",
        "# 8 을 수행하기 위한 참고 코드\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "\n",
        "\n",
        "\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
        "splits = text_splitter.split_documents(docs)\n",
        "vectorstore = Chroma.from_documents(documents=splits, embedding=OpenAIEmbeddings())\n",
        "\n",
        "retriever = vectorstore.as_retriever()\n",
        "prompt = hub.pull(\"rlm/rag-prompt\")\n",
        "def format_docs(docs):\n",
        "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
        "\n",
        "rag_chain = (\n",
        "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
        "    | prompt\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "for chunk in rag_chain.stream(\"What is Task Decomposition?\"):\n",
        "    print(chunk, end=\"\", flush=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yfTpQZoE_sUI",
        "outputId": "e92789b4-68cb-41f2-d1c6-0a45f9ef424b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Task Decomposition involves breaking down complex tasks into smaller, more manageable steps. Techniques like Chain of Thought (CoT) enhance model performance by guiding the model to think step by step, while the Tree of Thoughts expands on this by allowing multiple reasoning paths. The process can use simple prompts, task-specific instructions, or external tools like classical planners to decompose the tasks effectively."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def task10_run(\n",
        "    user_query: str = \"agent memory\",\n",
        "    verbose: bool = True,\n",
        "    chunk_size: int = 1800,\n",
        "    chunk_overlap: int = 250,\n",
        "    search_type: str = \"mmr\",            # \"similarity\" | \"mmr\"\n",
        "    search_kwargs: dict = None,          # 예: {\"k\": 12, \"lambda_mult\": 0.3}\n",
        "    proceed_if_any_yes: bool = True,     # True: 하나라도 yes면 진행 / False: 전부 yes여야 진행\n",
        "):\n",
        "    \"\"\"\n",
        "    End-to-end 실행:\n",
        "    1) 로드 → 2) 스플릿 → 3) 벡터스토어 → 4) 리트리브 → 5) 관련성 평가\n",
        "    6) (합격 시) 답변 생성 → 7) 환각 점검 → (필요 시 1회) 재생성 → 최종 출력\n",
        "\n",
        "    전역으로 필요한 것:\n",
        "    - ChatOpenAI, CHAT_MODEL\n",
        "    - task1_load, task2_split, task3_build_vectorstore, task4_retrieve\n",
        "    - build_relevance_chain, build_rag_answer_chain, build_hallucination_chain\n",
        "    - format_docs, unique_sources\n",
        "    \"\"\"\n",
        "    if search_kwargs is None:\n",
        "        search_kwargs = {\"k\": 12, \"lambda_mult\": 0.3} if search_type == \"mmr\" else {\"k\": 6}\n",
        "\n",
        "    llm = ChatOpenAI(model=CHAT_MODEL, temperature=0)\n",
        "    out = {\n",
        "        \"user_query\": user_query,\n",
        "        \"params\": {\n",
        "            \"chunk_size\": chunk_size,\n",
        "            \"chunk_overlap\": chunk_overlap,\n",
        "            \"search_type\": search_type,\n",
        "            \"search_kwargs\": search_kwargs,\n",
        "            \"proceed_if_any_yes\": proceed_if_any_yes,\n",
        "        },\n",
        "    }\n",
        "\n",
        "    # Task 1\n",
        "    if verbose: print(\"[Task1] Loading docs...\")\n",
        "    raw_docs = task1_load()\n",
        "    out[\"docs_count\"] = len(raw_docs)\n",
        "    if verbose: print(\"  loaded:\", out[\"docs_count\"])\n",
        "    if not raw_docs:\n",
        "        if verbose: print(\"No docs loaded. Stop.\")\n",
        "        out.update({\"chunks_count\": 0, \"retrieved_count\": 0, \"relevance_votes\": [], \"proceed\": False,\n",
        "                    \"final_answer\": None, \"sources\": [], \"hallucination_checks\": []})\n",
        "        return out\n",
        "\n",
        "    # Task 2\n",
        "    if verbose: print(\"[Task2] Splitting...\")\n",
        "    chunks = task2_split(raw_docs, chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
        "    out[\"chunks_count\"] = len(chunks)\n",
        "    if verbose: print(\"  chunks:\", out[\"chunks_count\"])\n",
        "    if not chunks:\n",
        "        if verbose: print(\"No chunks. Stop.\")\n",
        "        out.update({\"retrieved_count\": 0, \"relevance_votes\": [], \"proceed\": False,\n",
        "                    \"final_answer\": None, \"sources\": [], \"hallucination_checks\": []})\n",
        "        return out\n",
        "\n",
        "    # Task 3\n",
        "    if verbose: print(\"[Task3] Building vectorstore...\")\n",
        "    vs = task3_build_vectorstore(chunks)   # Chroma 0.4.x 이상은 persist() 불필요\n",
        "    if verbose: print(\"  vectorstore ready\")\n",
        "\n",
        "    # Task 4 (검색기 생성 + 검색)\n",
        "    if verbose: print(\"[Task4] Retrieving...\")\n",
        "    # 사용자 지정 search_type/kwargs 반영\n",
        "    retriever = vs.as_retriever(search_type=search_type, search_kwargs=search_kwargs)\n",
        "    retrieved = retriever.invoke(user_query)\n",
        "    out[\"retrieved_count\"] = len(retrieved)\n",
        "    if verbose: print(\"  retrieved:\", out[\"retrieved_count\"])\n",
        "    if out[\"retrieved_count\"] == 0:\n",
        "        if verbose: print(\"No retrieved docs. Stop.\")\n",
        "        out.update({\"relevance_votes\": [], \"proceed\": False,\n",
        "                    \"final_answer\": None, \"sources\": [], \"hallucination_checks\": []})\n",
        "        return out\n",
        "\n",
        "    # Task 5 (관련성 평가)\n",
        "    if verbose: print(\"[Task5] Relevance judging (per chunk)...\")\n",
        "    rel_chain = build_relevance_chain(llm)\n",
        "    rel_votes = []\n",
        "    for d in retrieved:\n",
        "        res = rel_chain.invoke({\"question\": user_query, \"context\": d.page_content})\n",
        "        rel_votes.append((res or {}).get(\"relevance\", \"no\"))\n",
        "    out[\"relevance_votes\"] = rel_votes\n",
        "    if verbose: print(\"  votes:\", rel_votes)\n",
        "\n",
        "    # 진행 조건: 하나라도 yes면 진행 / 전부 yes여야 진행\n",
        "    proceed = any(v == \"yes\" for v in rel_votes) if proceed_if_any_yes else all(v == \"yes\" for v in rel_votes)\n",
        "    out[\"proceed\"] = proceed\n",
        "    if not proceed:\n",
        "        if verbose: print(\"No relevant chunks by policy. Stop.\")\n",
        "        out.update({\"final_answer\": None, \"sources\": [], \"hallucination_checks\": []})\n",
        "        return out\n",
        "\n",
        "    # Task 8 (답변 생성)\n",
        "    if verbose: print(\"[Task8] Generating answer from retrieved context...\")\n",
        "    answer_chain = build_rag_answer_chain(llm)\n",
        "    context_text = format_docs(retrieved)\n",
        "    answer = answer_chain.invoke({\"context\": context_text, \"question\": user_query})\n",
        "    out[\"answer_v1\"] = answer or \"\"\n",
        "    if verbose: print(\"  answer_v1 length:\", len(out[\"answer_v1\"]))\n",
        "\n",
        "    # Task 9 (환각 판정)\n",
        "    if verbose: print(\"[Task9] Hallucination check...\")\n",
        "    h_chain = build_hallucination_chain(llm)\n",
        "    h1 = h_chain.invoke({\"answer\": out[\"answer_v1\"], \"context\": context_text}) or {}\n",
        "    out[\"hallucination_checks\"] = [h1]\n",
        "    if verbose: print(\"  check1:\", h1)\n",
        "\n",
        "    final_answer = out[\"answer_v1\"]\n",
        "\n",
        "    # Task 10 (필요시 1회 재생성)\n",
        "    if h1.get(\"hallucination\") == \"yes\":\n",
        "        if verbose: print(\"[Task10] Re-generate once (safer prompt)...\")\n",
        "        from langchain_core.prompts import PromptTemplate\n",
        "        from langchain_core.output_parsers import StrOutputParser\n",
        "        from langchain_core.runnables import RunnablePassthrough\n",
        "\n",
        "        cautious_prompt = PromptTemplate(\n",
        "            template=(\n",
        "                \"Answer strictly using only the facts present in the provided context. \"\n",
        "                \"If a detail is not explicitly mentioned, say you don't know.\\n\\n\"\n",
        "                \"Context:\\n{context}\\n\\nQuestion: {question}\"\n",
        "            ),\n",
        "            input_variables=[\"context\", \"question\"],\n",
        "        )\n",
        "        cautious_chain = (\n",
        "            {\"context\": RunnablePassthrough(), \"question\": RunnablePassthrough()}\n",
        "            | cautious_prompt\n",
        "            | ChatOpenAI(model=CHAT_MODEL, temperature=0)\n",
        "            | StrOutputParser()\n",
        "        )\n",
        "        answer2 = cautious_chain.invoke({\"context\": context_text, \"question\": user_query}) or \"\"\n",
        "        h2 = h_chain.invoke({\"answer\": answer2, \"context\": context_text}) or {}\n",
        "        out[\"answer_v2\"] = answer2\n",
        "        out[\"hallucination_checks\"].append(h2)\n",
        "        if verbose: print(\"  check2:\", h2)\n",
        "        final_answer = answer2\n",
        "\n",
        "    sources = unique_sources(retrieved)\n",
        "    out[\"final_answer\"] = final_answer\n",
        "    out[\"sources\"] = sources\n",
        "\n",
        "    if verbose:\n",
        "        print(\"\\n=== FINAL ANSWER ===\\n\")\n",
        "        print(final_answer)\n",
        "        print(\"\\n--- Sources ---\")\n",
        "        for i, s in enumerate(sources, 1):\n",
        "            print(f\"[{i}] {s}\")\n",
        "\n",
        "    return out\n"
      ],
      "metadata": {
        "id": "QZj5Kl8u_sWD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "CHAT_MODEL = \"gpt-4o\"\n",
        "EMBED_MODEL = \"text-embedding-3-small\"\n",
        "task10_run(user_query=\"agent memory\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HPUdQFVnADla",
        "outputId": "2f8e74ba-9fe9-430f-9687-db1a4fa29da2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Task1] Loading docs...\n",
            "  loaded: 3\n",
            "[Task2] Splitting...\n",
            "  chunks: 99\n",
            "[Task3] Building vectorstore...\n",
            "  vectorstore ready\n",
            "[Task4] Retrieving...\n",
            "  retrieved: 12\n",
            "[Task5] Relevance judging (per chunk)...\n",
            "  votes: ['yes', 'yes', 'yes', 'yes', 'no', 'no', 'no', 'no', 'no', 'no', 'no', 'yes']\n",
            "[Task8] Generating answer from retrieved context...\n",
            "  answer_v1 length: 1381\n",
            "[Task9] Hallucination check...\n",
            "  check1: {'hallucination': 'no'}\n",
            "\n",
            "=== FINAL ANSWER ===\n",
            "\n",
            "The context provided discusses the concept of agent memory within the framework of LLM-powered autonomous agents. In this system, memory is a crucial component that allows agents to retain and recall information over time, enabling them to behave in a manner conditioned by past experiences. There are two main types of memory mentioned:\n",
            "\n",
            "1. **Short-term memory**: This is akin to in-context learning, where the model uses the immediate context to learn and make decisions. It is limited in capacity, often constrained by a word limit, and is used for immediate tasks.\n",
            "\n",
            "2. **Long-term memory**: This type of memory allows the agent to store and retrieve information over extended periods. It often involves using an external database or vector store for efficient retrieval. Long-term memory is essential for maintaining a comprehensive record of the agent's experiences and interactions.\n",
            "\n",
            "The memory system is supported by a retrieval model that prioritizes information based on recency, importance, and relevance. Recent events are given higher scores, important memories are distinguished from mundane ones, and relevance is determined by how related the information is to the current situation or query.\n",
            "\n",
            "Overall, the memory component in LLM-powered agents is designed to enable them to learn from past actions, refine their strategies, and improve their performance over time.\n",
            "\n",
            "--- Sources ---\n",
            "[1] https://lilianweng.github.io/posts/2023-06-23-agent/\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'user_query': 'agent memory',\n",
              " 'params': {'chunk_size': 1800,\n",
              "  'chunk_overlap': 250,\n",
              "  'search_type': 'mmr',\n",
              "  'search_kwargs': {'k': 12, 'lambda_mult': 0.3},\n",
              "  'proceed_if_any_yes': True},\n",
              " 'docs_count': 3,\n",
              " 'chunks_count': 99,\n",
              " 'retrieved_count': 12,\n",
              " 'relevance_votes': ['yes',\n",
              "  'yes',\n",
              "  'yes',\n",
              "  'yes',\n",
              "  'no',\n",
              "  'no',\n",
              "  'no',\n",
              "  'no',\n",
              "  'no',\n",
              "  'no',\n",
              "  'no',\n",
              "  'yes'],\n",
              " 'proceed': True,\n",
              " 'answer_v1': \"The context provided discusses the concept of agent memory within the framework of LLM-powered autonomous agents. In this system, memory is a crucial component that allows agents to retain and recall information over time, enabling them to behave in a manner conditioned by past experiences. There are two main types of memory mentioned:\\n\\n1. **Short-term memory**: This is akin to in-context learning, where the model uses the immediate context to learn and make decisions. It is limited in capacity, often constrained by a word limit, and is used for immediate tasks.\\n\\n2. **Long-term memory**: This type of memory allows the agent to store and retrieve information over extended periods. It often involves using an external database or vector store for efficient retrieval. Long-term memory is essential for maintaining a comprehensive record of the agent's experiences and interactions.\\n\\nThe memory system is supported by a retrieval model that prioritizes information based on recency, importance, and relevance. Recent events are given higher scores, important memories are distinguished from mundane ones, and relevance is determined by how related the information is to the current situation or query.\\n\\nOverall, the memory component in LLM-powered agents is designed to enable them to learn from past actions, refine their strategies, and improve their performance over time.\",\n",
              " 'hallucination_checks': [{'hallucination': 'no'}],\n",
              " 'final_answer': \"The context provided discusses the concept of agent memory within the framework of LLM-powered autonomous agents. In this system, memory is a crucial component that allows agents to retain and recall information over time, enabling them to behave in a manner conditioned by past experiences. There are two main types of memory mentioned:\\n\\n1. **Short-term memory**: This is akin to in-context learning, where the model uses the immediate context to learn and make decisions. It is limited in capacity, often constrained by a word limit, and is used for immediate tasks.\\n\\n2. **Long-term memory**: This type of memory allows the agent to store and retrieve information over extended periods. It often involves using an external database or vector store for efficient retrieval. Long-term memory is essential for maintaining a comprehensive record of the agent's experiences and interactions.\\n\\nThe memory system is supported by a retrieval model that prioritizes information based on recency, importance, and relevance. Recent events are given higher scores, important memories are distinguished from mundane ones, and relevance is determined by how related the information is to the current situation or query.\\n\\nOverall, the memory component in LLM-powered agents is designed to enable them to learn from past actions, refine their strategies, and improve their performance over time.\",\n",
              " 'sources': ['https://lilianweng.github.io/posts/2023-06-23-agent/']}"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ]
    }
  ]
}